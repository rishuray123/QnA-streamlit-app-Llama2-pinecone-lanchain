# QnA-streamlit-app-Llama2-pinecone-langchain

# LLm powered applications

## LLAMA2
Llama 2 pretrained models are trained on 2 trillion tokens, and have double the context length than Llama 1. 
Its fine-tuned models have been trained on over 1 million human annotations.
We will use quantized versions of llama2 which we will download from hugging-face.


## Embeddings:
We will download embeddings model from hugging face (all-MiniLM-L6-v2) and use it generate embeddings for our query and our documents which we will then store in vector data base.
Similarity search:
We convert the query to vectors using our embedding model and compare it with the document vectors(again generated by same model and stored in vector DB), we check the cosine similarity scores and return the top k documents matching the query.

## Vector Databases
A vector database is a type of database that stores and manages unstructured data, such as text, images, or audio, in vector embeddings (high-dimensional vectors) to make it easy to find and retrieve similar objects quickly.
PineCone is one such vector store which helps storage,retrieval and search of vector databases easy. We will use PineCone in this project.

## Langchain
LangChain is a framework for developing applications powered by language models. It enables applications that are:
Data-aware: connect a language model to other sources of data
Agentic: allow a language model to interact with its environment

The main value props of LangChain are:
Components: abstractions for working with language models, along with a collection of implementations for each abstraction. Components are modular and easy-to-use, whether you are using the rest of the LangChain framework or not
Off-the-shelf chains: a structured assembly of components for accomplishing specific higher-level tasks.

Here, we will use llama2, pinecone and langchain to create an assistant which can answer any questions from your document.
Refer the notebook for it.

## Streamlit-app
We will also create a streamlit app for a GUI where you can upload your doc and chat with your document on your browser.

## Latency
1. Chat might take some time as we are running it on CPU and have not optimized for run-time.
2. Semantic Search is almost real-time.
